{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wbepu01LHJoL"
   },
   "source": [
    "# Trabajo Preparacion, visualizacion de Datos y Machine learning con Python<a class=\"tocSkip\">\n",
    "## Ciencia de datos en Produccion <a class=\"tocSkip\">\n",
    "\n",
    "**Estudiante:** Sebastián Cardona y Jose Miguel Millán\n",
    "\n",
    "**ID:** 1094910122 y 1088334182\n",
    "\n",
    "**Email:** sacardonar@uqvirtual.edu.co y josem.millanl@uqvirtual.edu.co\n",
    "\n",
    " \n",
    "Docente: [Jose R. Zapata](https://joserzapata.github.io)\n",
    "- https://joserzapata.github.io\n",
    "- https://twitter.com/joserzapata\n",
    "- https://www.linkedin.com/in/jose-ricardo-zapata-gonzalez/       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo del Trabajo <a class=\"tocSkip\">\n",
    "En el anterior experimento se entrenaron dos modelos de regresión considerando unos casos atípicos dentro del análisis, los modelos resultantes tuvieron un bajo desempeño, por lo tanto, en este experimento se eliminarán los datos atípicos encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XE2q_0ARHJoM"
   },
   "source": [
    "# Definir el Problema a Resolver\n",
    "\n",
    "El dataset \"house data\", inicialmente se realizará una exploración de datos, para poder saber la calidad del dataset, iniciando con una limpieza la cual consta de eliminar duplicados, identificación de datos atípicos, nullos o mal escritos para poder tratarlos y mitigarlos, ya sea con la eliminación o aplicación de métodos estadísticos, con la finalidad de tener un datset listo y poder aplicar una regresión lineal y poder predecir los precios de venta de una casa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fWGXusiHJoO"
   },
   "source": [
    "## Describir los datos de entrada y salida\n",
    "- Cantidad de Variables\n",
    "- Tipo de Variables\n",
    "- Significado de cada Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjPWhesSHJoO"
   },
   "source": [
    "# Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksQYJRZTHJoP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from pandas_profiling.profile_report import ProfileReport\n",
    "from sklearn.linear_model import SGDRegressor, Lasso, Ridge, LinearRegression, ElasticNet\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, cross_validate, RepeatedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import phik\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data.preprocessing import preprocessing\n",
    "from src.features.build_features import build_features\n",
    "from src.jutils.data import DataUtils\n",
    "from src.jutils.visual import Plot\n",
    "from src.data.procesamiento_datos import Preprocesamiento, LimpiezaCalidad, ProcesamientoDatos\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Funciones utilizadas para realizar un preprocesamiento general.\n",
    "\n",
    "def validar_duplicados(_df):\n",
    "    _filas = _df.shape[0]\n",
    "    _cant_duplicados = _df.duplicated().sum()\n",
    "    print(f'De {_filas} registros hay {_cant_duplicados} filas duplicadas, representando el {_cant_duplicados/_filas:.2%}')\n",
    "\n",
    "def eliminar_duplicados(_df):\n",
    "    # Eliminando duplicados\n",
    "    _df = _df.drop_duplicates(keep='first')\n",
    "    _filas = _df.shape[0]\n",
    "    print(f'Después de la eliminación de duplicados, el conjunto de datos queda con {_filas} filas.')\n",
    "    return _df\n",
    "\n",
    "def validar_index_duplicados(_df):\n",
    "    # Validando duplicados de index\n",
    "    _son_duplicados = _df['index'].duplicated()\n",
    "    _cant_duplicados = _son_duplicados.sum()\n",
    "    _filas = _df.shape[0]\n",
    "    print(f'De {_filas} registros, hay {_cant_duplicados} registros con index duplicado, que representan el {_cant_duplicados/_filas:.2%}.')\n",
    "    return _son_duplicados\n",
    "\n",
    "def convertir_col_date_a_date(_df):\n",
    "    _df['date'] = pd.to_datetime(_df['date'], errors='coerce')\n",
    "    return _df\n",
    "\n",
    "def reemplazar_valores_extremos(_df, _columnas_numericas):\n",
    "    _df[_columnas_numericas] = _df[_columnas_numericas].where(lambda x: x > -1e+10, other=np.nan).where(\n",
    "        lambda x: x < 1e+10, other=np.nan)\n",
    "    return _df\n",
    "\n",
    "def reemplazar_nulos_por_la_media(_df, _columnas_numericas):\n",
    "    # Se reemplazan los valores nulos por la media Nota: No se considera que haya data leakage pues los valores\n",
    "    # reemplazados son entre registros con el mismo index y como al final se va a dejar un dataset con index únicos,\n",
    "    # no hay riesgo que estén tanto en el set de entrenamiento como en el de test\n",
    "    for columna_numerica in _columnas_numericas:\n",
    "        _df[columna_numerica] = _df[columna_numerica].fillna(\n",
    "            _df.groupby('index')[columna_numerica].transform('median'))\n",
    "    return _df\n",
    "\n",
    "def reemplazar_fechas_nulas(_df):\n",
    "    # Reemplazando fechas nulas por la primera fecha no nula\n",
    "    _df['date'] = _df['date'].fillna(\n",
    "        _df.groupby(['index'], sort=False)['date'].apply(lambda x: x.ffill().bfill()))\n",
    "    return _df\n",
    "\n",
    "def reemplazar_ceros_por_nulos(_df):\n",
    "    # Reemplazando ceros por valores nulos\n",
    "    _df[['sqft_basement', 'yr_renovated']] = _df[['sqft_basement', 'yr_renovated']].replace(0, np.nan)\n",
    "    return _df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones utilizadas para procesar únicamente los datos de entrenamiento\n",
    "def z_score_outliers(_df, _column):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        zscore, outlier\n",
    "    \"\"\"\n",
    "    # Adaptado de https://www.kaggle.com/code/shweta2407/regression-on-housing-data-accuracy-87\n",
    "    #creating lists to store zscore and outliers \n",
    "    zscore = []\n",
    "    isoutlier =[]\n",
    "    # for zscore generally taken thresholds are 2.5, 3 or 3.5 hence i took 3\n",
    "    threshold = 3\n",
    "    # calculating the mean of the passed column\n",
    "    mean = np.mean(_df[_column])\n",
    "    # calculating the standard deviation of the passed column\n",
    "    std = np.std(_df[_column])\n",
    "    for i in _df[_column]:\n",
    "        z = (i-mean)/std\n",
    "        zscore.append(z)\n",
    "        #if the zscore is greater than threshold = 3 that means it is an outlier\n",
    "        isoutlier.append(np.abs(z) > threshold)\n",
    "    return zscore, isoutlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para realizar el procesamiento de los datos antes de ingresar al modelo\n",
    "def mediana_recortada_imputacion(_df, _column, _isoutlier):\n",
    "    mediana_recortada = _df[_column][~_isoutlier].median()\n",
    "    _df.loc[_isoutlier, _column] = mediana_recortada\n",
    "    return _df\n",
    "\n",
    "def calculo_variables_adicionales(_df):\n",
    "    _df['tiene_sotano'] = (~_df['sqft_basement'].isna()).astype(int)\n",
    "    _df['fue_renovada'] = (~_df['yr_renovated'].isna()).astype(int)\n",
    "    _df['yr_date'] = _df['date'].dt.year\n",
    "    _df['antiguedad_venta'] = _df['yr_date'] - _df['yr_built']\n",
    "    return _df\n",
    "\n",
    "def procesamiento_datos_faltantes(_df, _columnas):\n",
    "    _df = _df.dropna(subset=_columnas)\n",
    "    return _df\n",
    "\n",
    "def clasificar_columnas(_df, _clasificacion_columnas):\n",
    "    _df[_clasificacion_columnas['categorica_ordinal']] = _df[_clasificacion_columnas['categorica_ordinal']].astype(int)\n",
    "    _df[_clasificacion_columnas['numerica_continua']] = _df[_clasificacion_columnas['numerica_continua']].astype(float)\n",
    "    _df[_clasificacion_columnas['numerica_discreta']] = np.floor(_df[_clasificacion_columnas['numerica_discreta']])\n",
    "    return _df\n",
    "    \n",
    "def imputacion_de_datos(_df, _columnas):\n",
    "    _df[_columnas] = _df[_columnas].fillna(0)\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profiler_to_file(_profiler, archivo):\n",
    "    print('Ejecutando profiler')\n",
    "    _profiler.to_file(du.data_folder_path.parent.joinpath('reports/' + archivo))\n",
    "    return True\n",
    "\n",
    "def calcular_descriptivas(_df):\n",
    "    descriptivas = _df.describe()\n",
    "    descriptivas.loc['rango'] = descriptivas.loc['max'] - descriptivas.loc['min']\n",
    "    descriptivas.loc['IQR'] = descriptivas.loc['75%'] - descriptivas.loc['25%']\n",
    "    descriptivas.loc['coef de var'] = descriptivas.loc['std']/descriptivas.loc['mean']\n",
    "    descriptivas.loc['skewness'] = du.data.skew(numeric_only=True)\n",
    "    descriptivas.loc['kurtosis'] = du.data.kurtosis(numeric_only=True)\n",
    "    return descriptivas\n",
    "\n",
    "plot = Plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LM4dC7lBHJoT"
   },
   "source": [
    "# Cargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2C-zrtBHJoT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du = DataUtils(\n",
    "    Path(r'..\\data').resolve().absolute(),\n",
    "    \"kc_house_dataDS.parquet\",\n",
    "    'price',\n",
    "    lambda path: pd.read_parquet(path),\n",
    "    lambda df, path: df.to_parquet(path)\n",
    ")\n",
    "du.data = du.load_data(du.interim_path.joinpath(du.input_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripcion General del Dataset\n",
    "- numero de filas y columnas\n",
    "- tipos de datos y si estan correctos\n",
    "\n",
    "Durante la exploración inicial se realizó la conversión de los tipos de datos, y la correcta representación de datos nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shape = du.data.shape\n",
    "filas = shape[0]\n",
    "columnas = shape[1]\n",
    "print(f'El conjunto de datos se compone de {filas} filas y {columnas} columnas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las columnas son del tipo correcto a excepción de date, se deberá hacer la conversión de este campo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de calidad de datos general\n",
    "- Filas con valores exactamente iguales (duplicados)\n",
    "- Columnas duplicadas\n",
    "- Columnas con valores constantes o sin informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validar_duplicados(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data = eliminar_duplicados(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validar indices duplicados\n",
    "son_duplicados = validar_index_duplicados(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Revisando los primeros registros duplicados\n",
    "du.data[son_duplicados].sort_values(by='index').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisando los registros duplicados por index, se encuentra que muchas columnas tienen los mismos valores , lo único que cambia es que hay algunos faltantes y hay otros valores extremadamente bajos o altos, adicionalmente se observan algunos registros de la columna date que no son fechas. Primero se convertirá los valores de la columna date a date y los que no puedan ser convertidos se reemplazarán por valores nulos, luego se reemplazarán los valores extremos por valores nulos, luego se calculará la mediana por index para las columnas numéricas y se reemplazarán los valores nulos por estas medianas. Luego se eliminarán filas duplicadas y se reevaluarán los index duplicados.\n",
    "\n",
    "Si nuestra suposición es correcta, no importa realizar una imputación por la mediana pues todos los valores de los índices son iguales, luego de hacer la imputación y eliminar nuevamente duplicados, no deberían quedar índices duplicados, en caso de que sigan habiendo índices duplicados se deben revertir las imputaciones realizadas y buscar otra estrategia para eliminar duplicados exactos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convirtiendo la columna date a datetime\n",
    "du.data = convertir_col_date_a_date(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reemplazando valores extremos, menores a -1e+10 o mayores a 1e+10\n",
    "columnas_numericas = [columna for columna in du.data.columns if columna != 'date']\n",
    "du.data = reemplazar_valores_extremos(du.data, columnas_numericas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se reemplazan los valores extremos por la media\n",
    "# Nota: No se considera que haya data leakage pues los valores reemplazados son entre registros con el mismo index y como \n",
    "# al final se va a dejar un dataset con index únicos, no hay riesgo que estén tanto en el set de entrenamiento como en el de\n",
    "# test\n",
    "du.data = reemplazar_nulos_por_la_media(du.data, columnas_numericas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reemplazando fechas nulas por la primera fecha no nula\n",
    "du.data = reemplazar_fechas_nulas(du.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las siguientes columnas, un cero representa un dato nulo, por lo tanto se reemplazarán.\n",
    "- sqft_basement\n",
    "- yr_renovated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reemplazando ceros por valores nulos\n",
    "du.data = reemplazar_ceros_por_nulos(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validar_duplicados(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data = eliminar_duplicados(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "son_duplicados = validar_index_duplicados(du.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez validados los índices duplicados, se evidencia que la limpieza surtió efecto (Se debe implementar un control que valide esto cuando se vaya a realizar un reentrenamiento, se debe alertar cuando sigan habiendo índices duplicados e interrumpir el proceso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validando columnas con valores constantes\n",
    "unicos=du.data.nunique()\n",
    "unicos[unicos==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna wertyj tiene valores constantes, por lo tanto se eliminará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data = du.data.drop(columns=list(unicos[unicos==1].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nulos = du.data.isnull().sum()\n",
    "cant_unicos = du.data.apply(lambda x: len(x.unique()))\n",
    "porce = nulos/filas\n",
    "nulos = pd.DataFrame({'nulos':nulos, 'porc':porce, 'cant_unicos': cant_unicos})\n",
    "# Se contarán las filas que contengan algún dato nulo\n",
    "al_menos_un_nulo=du.data.isnull().any(axis=1).sum()\n",
    "nulos.sort_values(by='porc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'De {filas} registros, hay {al_menos_un_nulo} registros con al menos un valor nulo, representando el {al_menos_un_nulo/filas:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe tener en cuenta que para el caso de yr_renovated y sqft_basement, un valor nulo no representa necesariamente falta de información, para el caso de yr_renovated, un nulo representa que esa casa nunca se renovó. Y en el caso de sqft_basement quiere decir que la casa no tiene sótano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando variables adicionales\n",
    "du.data = calculo_variables_adicionales(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta vez no se tendrán en cuenta las columnas yr_renovated y sqft_basement\n",
    "df = du.data.drop(columns=['yr_renovated', 'sqft_basement'])\n",
    "nulos = df.isnull().sum()\n",
    "cant_unicos = df.apply(lambda x: len(x.unique()))\n",
    "porce = nulos/filas\n",
    "nulos = pd.DataFrame({'nulos':nulos, 'porc':porce, 'cant_unicos': cant_unicos})\n",
    "# Se contarán las filas que contengan algún dato nulo\n",
    "al_menos_un_nulo=df.isnull().any(axis=1).sum()\n",
    "nulos.sort_values(by='porc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'De {filas} registros, hay {al_menos_un_nulo} registros con al menos un valor nulo, representando el {al_menos_un_nulo/filas:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el entrenamiento del primer modelo se eliminarán los datos nulos debido a su poca cantidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_eliminar_nulos = du.data.drop(columns=['yr_renovated', 'sqft_basement']).columns\n",
    "du.data = procesamiento_datos_faltantes(du.data, columnas_a_eliminar_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'{du.data.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el dataset en Training set y Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_test, validation = train_test_split(du.data, test_size=0.2, random_state=1)\n",
    "du.save_data(train_test, du.raw_train_test_path)\n",
    "du.save_data(validation, du.raw_validation_path)\n",
    "print(f'{train_test.shape=}')\n",
    "print(f'{validation.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gnmm4vdHJoV"
   },
   "source": [
    "# Descripcion  y Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYzna68bHJoW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data = du.load_data(du.raw_train_test_path)\n",
    "print('Tipos de variables')\n",
    "du.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFSKQY4Amzwv"
   },
   "source": [
    "## Identificacion de Variables\n",
    "- Variables de entrada y de salida\n",
    "- Tipo de Variables (categoricas o Numericas)\n",
    "- Tipo de datos (int, float, string, factor, boolean, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bnXkopIFHJob",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clasificación de columnas\n",
    "clasificacion_columnas = {\n",
    "    'categorica_ordinal': ['zipcode', 'grade', 'view', 'waterfront', 'condition', 'lat', 'long'],\n",
    "    'fecha': ['date'],\n",
    "    'id': ['index'],\n",
    "    'numerica_continua': ['sqft_basement', 'sqft_above', 'sqft_living15', 'sqft_lot', 'price', 'sqft_lot15', 'sqft_living'],\n",
    "    'numerica_discreta': ['bathrooms', 'bedrooms', 'yr_renovated', 'yr_built', 'jhygtf', 'yr_date', 'antiguedad_venta', 'floors']\n",
    "}\n",
    "columna_salida = 'price'\n",
    "columnas_a_descartar = ['date', 'index']\n",
    "\n",
    "du.data = du.data.drop(columns=columnas_a_descartar)\n",
    "\n",
    "columnas_entrada = du.data.drop(columns=columna_salida).columns\n",
    "print(f'Columna salida: {columna_salida}')\n",
    "print(f'Columnas de entrada: {columnas_entrada}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis General Univariable y Bivariable \n",
    "Analisis de cada una de las variables para lograr calidad de datos en cada columna\n",
    "- **Correccion del tipo de dato (numericas, categoricas, string) de cada columna (optimizar memoria)**\n",
    "- Deteccion de numero de datos faltantes\n",
    "- Deteccion de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.data= clasificar_columnas(du.data, clasificacion_columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminación de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero se analizará la distribución de cada variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plots = [plot.box(du.data, y=columna) for columna in clasificacion_columnas['numerica_continua']]\n",
    "histograms = [plot.histogram(du.data, x=columna, text_auto=False) for columna in clasificacion_columnas['numerica_continua']]\n",
    "\n",
    "plot.grid_subplot(*box_plots, cols= 3, title= 'Distribución inicial',titles=clasificacion_columnas['numerica_continua']).show()\n",
    "plot.grid_subplot(*histograms, cols= 3, title= 'Distribución inicial',titles=clasificacion_columnas['numerica_continua']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminando registros identificados como outliers según el z_score\n",
    "\n",
    "du.data = du.data[~pd.Series(z_score_outliers(du.data, 'price')[1], index=du.data.index)]\n",
    "du.data = du.data[~pd.Series(z_score_outliers(du.data, 'sqft_lot')[1], index=du.data.index)]\n",
    "du.data = du.data[~pd.Series(z_score_outliers(du.data, 'sqft_lot15')[1], index=du.data.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "box_plots = [plot.box(du.data, y=columna) for columna in clasificacion_columnas['numerica_continua']]\n",
    "histograms = [plot.histogram(du.data, x=columna, text_auto=False) for columna in clasificacion_columnas['numerica_continua']]\n",
    "\n",
    "plot.grid_subplot(*box_plots, cols= 3, title= 'Distribución inicial',titles=clasificacion_columnas['numerica_continua']).show()\n",
    "plot.grid_subplot(*histograms, cols= 3, title= 'Distribución inicial',titles=clasificacion_columnas['numerica_continua']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las variables numericas_discreta se analizarán sus rangos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'min':du.data[clasificacion_columnas['numerica_discreta']].min(),\n",
    "    'max':du.data[clasificacion_columnas['numerica_discreta']].max(),\n",
    "    'nulos':du.data[clasificacion_columnas['numerica_discreta']].isna().sum()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definicion de outliers:\n",
    "\n",
    "Se considerarán outliers:\n",
    "- Apartamentos con bathrooms o bedrooms igual a 0\n",
    "- Apartamentos con bedrooms mayor a 5\n",
    "- Apartamentos con bathrooms mayor a 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bathrooms_outliers = (du.data['bathrooms']==0) | (du.data['bathrooms'] > 4)\n",
    "bedrooms_outliers = (du.data['bedrooms']==0) | (du.data['bedrooms'] > 5)\n",
    "son_outliers = bathrooms_outliers | bedrooms_outliers\n",
    "cant_outliers = son_outliers.sum()\n",
    "print(f'Con estas características hay: {cant_outliers} outliers representando el {cant_outliers/len(son_outliers):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputación de outliers: \n",
    "Se reemplazarán los outliers calculando la mediana recortada la cual se realiza teniendo en cuenta únicamente los inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data = mediana_recortada_imputacion(du.data, 'bathrooms', bathrooms_outliers)\n",
    "du.data = mediana_recortada_imputacion(du.data, 'bedrooms', bedrooms_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisando los datos después de la imputación\n",
    "plot.grid_subplot(\n",
    "    *[plot.bar(du.data, x=column, max_bins=10) for column in clasificacion_columnas['numerica_discreta']], \n",
    "    cols=3, \n",
    "    titles=clasificacion_columnas['numerica_discreta']).show()\n",
    "pd.DataFrame({\n",
    "    'min':du.data[clasificacion_columnas['numerica_discreta']].min(),\n",
    "    'max':du.data[clasificacion_columnas['numerica_discreta']].max(),\n",
    "    'nulos':du.data[clasificacion_columnas['numerica_discreta']].isna().sum()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para las columnas categóricas se revisarán sus valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'distinct_count':du.data[clasificacion_columnas['categorica_ordinal']].nunique(),\n",
    "    'nulos':du.data[clasificacion_columnas['categorica_ordinal']].isna().sum()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se evidencian datos atípicos. lat, long y zipcode tienen demasiada cardinalidad pero es normal por ser datos de ubicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5CEar_Cnw2x"
   },
   "source": [
    "## Eliminar columnas de datos Innecesarios\n",
    "\n",
    "Se realizará un perfilado de los datos para identificar problemas de calidad no identificados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxoNbnANoJt8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profiler = ProfileReport(du.data, explorative=True)\n",
    "\n",
    "profiler_to_file(profiler, '2_0_0 Perfilado inicial.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utilizará el índice de correlación phik pues este permite calcular la correlación entre variables numéricas y categóricas al tiempo.\n",
    "# Este indice funciona mejor cuando no hay valores nulos, por lo tanto se reemplazarán los valores nulos por 0\n",
    "cor_mat = du.data.fillna(0).phik_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero se analizará la correlación entre las variables de entrada y se descartarán aquellas con una correlación superior a 0.9\n",
    "\n",
    "(cor_mat.loc[columnas_entrada, columnas_entrada] > 0.9).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se analizarán con mas detalle aquellas que tienen una alta correlación con más de 1 columna (Consigo misma)\n",
    "columnas_alta_correlacion = ['yr_renovated', 'fue_renovada', 'jhygtf', 'yr_built', 'tiene_sotano', 'sqft_living', 'antiguedad_venta', 'sqft_above', 'sqft_basement']\n",
    "px.imshow(\n",
    "    cor_mat.loc[columnas_alta_correlacion, columnas_alta_correlacion].round(2), \n",
    "    color_continuous_scale= 'blues', \n",
    "    text_auto=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera columna a eliminar es **jhygtf** se observa que es la misma variable que **yr_renovated**. Aunque la columna **fue_renovada** está calculada con base en **yr_renovated** se conservarán ambas para posteriormente elegir con cual de las dos se puede obtener un mejor modelo.\n",
    "\n",
    "Quedan altas correlaciones entre las siguientes columnas:\n",
    "- yr_build y antiguedad_venta\n",
    "- tiene_sotano y sqft_basement\n",
    "- sqft_living y sqft_above\n",
    "- sqft_living y sqft_basement\n",
    "\n",
    "Para las restantes, se conservarán todas para después mediante el cálculo de la importancia de variables escoger cual tiene un mejor poder predictivo.\n",
    "\n",
    "Entonces solo se eliminará la columna **jhygtf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.data = du.data.drop(columns=['jhygtf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlación de todas las variables de entrada con respecto a la salida\n",
    "cor_mat.loc['price', du.data.columns].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de variables\n",
    "Se calculará la importancia de las variables de entrada con respecto al precio adicionando una columna dummy que contrendrá valores aleatorios y se eliminarán aquellas variables cuya importancia sea inferior a la columna aleatoria (Al ser esta aleatoria, sabemos desde el principio que esta no puede ser una buena predictora).\n",
    "\n",
    "Para realizar este calculo se utilizará un regresor lineal, se debe escalar primero las variables de entrada para poder comparar sus coeficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_features = make_pipeline(StandardScaler(), LinearRegression())\n",
    "X = du.data.fillna(0).drop(columns='price')\n",
    "X['random'] = np.random.normal(size=(X.shape[0], 1))\n",
    "y = du.data['price']\n",
    "\n",
    "cv_model = cross_validate(\n",
    "   pipeline_features, X, y, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "   return_estimator=True, n_jobs=2\n",
    ")\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "   [model[1].coef_ for model in cv_model['estimator']],\n",
    "   columns=X.columns\n",
    ")\n",
    "print()\n",
    "print(f'mean_test_score: {np.mean(cv_model[\"test_score\"])}')\n",
    "print(f'std_test_score : {np.std(cv_model[\"test_score\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(coefs, orientation='h', title='Importancia de los coeficientes y sus variaciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debido a que no se normalizó el precio, se dividirán los coeficientes por la media del precio para tenerlos en una escala mas\n",
    "# manejable\n",
    "\n",
    "coefs_resumen = pd.DataFrame({\n",
    "    'variacion': (coefs.std()/du.data['price'].mean()),\n",
    "    'media': coefs.mean()/du.data['price'].mean(),\n",
    "    'media_absoluta': coefs.abs().mean()/du.data['price'].mean()\n",
    "})\n",
    "coef_variacion = coefs_resumen['variacion'].sort_values(ascending=False)\n",
    "coef_variacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aquellas variables con demasiada variación no es confiable tomar el promedio como su importancia, por lo tanto no serán eliminadas.\n",
    "\n",
    "Se analizarán aquellas con una variación inferior a 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_confiables = coef_variacion[coef_variacion <= 0.7]\n",
    "coefs_confiables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(coefs[coefs_confiables.index], orientation='h', title='Importancia de los coeficientes y sus variaciones').show()\n",
    "px.box(coefs.abs()[coefs_confiables.index], orientation='h', title='Importancia absoluta de los coeficientes y sus variaciones').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma se pueden eliminar aquellas que sean consistentemente peor que la columna random.\n",
    "\n",
    "Al analizar el gráfico, no se pueden identificar variables con un peor desempeño que random.\n",
    "\n",
    "Se usará f_regression de scikit learn para calcular la importancia de las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "f_statistic, p_values =  f_regression(X, y)\n",
    "pd.DataFrame({'f_statistic': f_statistic, 'p_value': p_values.round(3)}, index=X.columns).sort_values(by='f_statistic', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta técnica, se puede determinar el umbral desde el que se van a descartar las variables, se observa que ninguna variable tiene un poder predictivo peor que la variable random.\n",
    "\n",
    "Pero se puede ver que la importancia de las variables **lat**, **long** y **yr_date** no son confiables al tener un p-value mayor a 0.05\n",
    "\n",
    "Adicionalmente, podemos determinar que de las variables de entrada altamente correlacionadas, se pueden eliminar las siguientes\n",
    "- antiguedad_venta > yr_built\n",
    "- sqft_basement > tiene_sotano\n",
    "- sqft_living > sqft_above\n",
    "- sqft_living > sqft_basement\n",
    "\n",
    "Para **yr_renovated** y **fue_renovada** la diferencia es muy poca, por lo tanto se escogerá **fue_renovada** por la alta cantidad de nulos de **yr_renovated**\n",
    "\n",
    "\n",
    "Por lo tanto se descartarán las siguientes columnas:\n",
    "\n",
    "**lat**, **long**, **yr_date**, **yr_built**, **tiene_sotano**, **sqft_above**, **sqft_basement**, **yr_built** y **yr_renovated**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.data = du.data.drop(columns=['lat', 'long', 'yr_date', 'yr_built', 'tiene_sotano', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LmnrQtbHJox"
   },
   "source": [
    "## Procesamiento de Datos Faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que se eliminaron las columnas **yr_renovated** y **sqft_basement**, se puede omitir este paso pues ya no quedan mas nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFwqgwOhHJox",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnkcN0goHJod"
   },
   "source": [
    "## Analisis Univariable\n",
    "\n",
    "Estadistico Descriptico y Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEvH4-LGHJod"
   },
   "source": [
    "### Variables Numericas\n",
    "\n",
    "| Tendencia Central |   Medida de Dispersión    | Visualizacion |\n",
    "|:-----------------:|:-------------------------:|:-------------:|\n",
    "|       Media       |           Rango           |  Histogramas  |\n",
    "|      Mediana      |         Cuartiles         |   Boxplots    |\n",
    "|       Moda        | Rango inter cuartil (IQR) |               |\n",
    "|      Minimo       |         Varianza          |               |\n",
    "|      Maximo       |   Desviacion Estandard    |               |\n",
    "|         .         |         Skewness          |               |\n",
    "|         .         |         Kurtosis          |               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDTBsGi0HJoe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calcular_descriptivas(du.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizando la clasificación de columnas para dejar solo las que están en el dataframe\n",
    "clasificacion_columnas['numerica_continua'] = list(set(clasificacion_columnas['numerica_continua']).intersection(du.data.columns))\n",
    "clasificacion_columnas['categorica_ordinal'] = list(set(clasificacion_columnas['categorica_ordinal']).intersection(du.data.columns))\n",
    "clasificacion_columnas['numerica_discreta'] = list(set(clasificacion_columnas['numerica_discreta']).intersection(du.data.columns))\n",
    "\n",
    "\n",
    "columnas_a_graficar = clasificacion_columnas['numerica_continua']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots1 = [plot.box(du.data, y=variable_numerica) for variable_numerica in columnas_a_graficar] \n",
    "plot.grid_subplot(*plots1, cols=3, title='Diagramas de cajas y bigotes', titles=columnas_a_graficar).show()\n",
    "\n",
    "plots2 = [plot.histogram(du.data, x=variable_numerica) for variable_numerica in columnas_a_graficar]\n",
    "plot.grid_subplot(*plots2, cols=3, title='Histogramas', titles=columnas_a_graficar).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observan datos asimétricos para todas las columnas numéricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXQHmtMbHJoh"
   },
   "source": [
    "### Variables Categoricas\n",
    "- Numero de elementos por categoria\n",
    "- Porcentaje de elementos por categoria\n",
    "- Graficos de barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPY0sHqNHJoi",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumen = []\n",
    "\n",
    "for variable_categorica in clasificacion_columnas['categorica_ordinal']:\n",
    "    col = du.data[variable_categorica]\n",
    "    elms_cat = col.groupby(by=col).agg('count')\n",
    "    total = elms_cat.sum()\n",
    "    porc = elms_cat / total\n",
    "    porc.name = 'porc'\n",
    "    df = pd.DataFrame([elms_cat, porc]).transpose()\n",
    "    resumen.append(df)\n",
    "\n",
    "for tabla in resumen:\n",
    "    display(HTML(tabla.to_html()))\n",
    "    variable = tabla.columns[0]\n",
    "    fig = px.bar(tabla[variable], orientation='h', title=str(variable))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmf_f9TOHJok"
   },
   "source": [
    "## Analisis Bivariable\n",
    "Estadistico Descriptico y Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aVUkwJVHJom"
   },
   "source": [
    "### Numericas vs Numericas\n",
    "- Scatter Plot\n",
    "- Heatmap\n",
    "- Correlacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_graficar = list(filter(lambda x: x!='price', clasificacion_columnas['numerica_continua']))\n",
    "\n",
    "plots = [plot.scatter(du.data, x=columna, y='price') for columna in columnas_a_graficar]\n",
    "\n",
    "plot.grid_subplot(*plots, cols = 2, titles=columnas_a_graficar).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificacion_columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_a_graficar = clasificacion_columnas['categorica_ordinal'] + clasificacion_columnas['numerica_discreta']\n",
    "nbins = [0 if len(du.data[x].unique()) < 20 else 20 for x in columnas_a_graficar]\n",
    "\n",
    "plots = [\n",
    "    plot.box(du.data, x=column, y='price', nbins=nbin)\n",
    "    for column, nbin in zip(columnas_a_graficar, nbins)\n",
    "]\n",
    "plot.grid_subplot(*plots, \n",
    "                  cols=2, \n",
    "                  titles=columnas_a_graficar, \n",
    "                  title='Box plot entre entradas categóricas y precio',\n",
    "                  height=1500\n",
    "                 ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las variables categóricas, las que parecen tener un mayor impacto en el precio son grade, view y y waterfront.\n",
    "La variable condicion parece tener un aumento en el precio cuando es mayor a 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "067r3JdGHJon",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "corr_matrix: DataFrame = du.data.phik_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables relacionadas con el tamaño del apartamento y la calificación tienen mayor correlación con el precio del apartamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ordenando matriz de correlación con respecto a precio\n",
    "corr_matrix = corr_matrix.sort_values(by='price', ascending=False)\n",
    "corr_matrix = corr_matrix.reindex(columns=corr_matrix.index)\n",
    "sns.heatmap(corr_matrix, cmap='PuOr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columnas = ['sqft_living', 'sqft_above', 'sqft_basement', 'antiguedad_venta', 'yr_built', 'yr_date', 'yr_renovated', \n",
    "            'fue_renovada', 'grade', 'sqft_living15', 'sqft_lot', 'tiene_sotano', 'condition', 'floors', 'bathrooms', \n",
    "            'view', 'bedrooms', 'waterfront', 'sqft_lot15', 'zipcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profiler2 = ProfileReport(du.data, explorative=True)\n",
    "profiler_to_file(profiler2, '2_0_1 Perfilado de datos.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calcular_descriptivas(du.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6PpnSEDHJpY"
   },
   "source": [
    "## Analisis Univariable y Bivariable Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profiler3 = ProfileReport(du.data, explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "profiler_to_file(profiler3, '2_0_2 Transformacion_final.html')\n",
    "profiler3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "du.data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRwep7LjHJpZ"
   },
   "source": [
    "### Variables de entrada\n",
    "- zipcode\n",
    "- grade\n",
    "- view\n",
    "- bathrooms\n",
    "- bedrooms\n",
    "- sqft_living15\n",
    "- waterfront\n",
    "- floors\n",
    "- sqft_lot\n",
    "- condition\n",
    "- sqft_lot15\n",
    "- sqft_living\n",
    "- fue_renovada\n",
    "- antiguedad_venta\n",
    "\n",
    "\n",
    "### Variables de salida\n",
    "- price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELAMIENTO DE LOS DATOS (MACHINE LEARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variables_entrada = ['zipcode', 'grade', 'view', 'bathrooms', 'bedrooms', 'sqft_living15', 'waterfront', 'floors', 'sqft_lot', 'condition', 'sqft_lot15', 'sqft_living', 'fue_renovada', 'antiguedad_venta']\n",
    "variable_salida  = 'price'\n",
    "du.data = du.data[variables_entrada + [variable_salida]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_name = 'price'\n",
    "x_names = [columna for columna in du.data.columns if not columna == 'price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion y Evaluacion Cruzada (k-fold Cross Validation)\n",
    "\n",
    "Se hace seleccion de los mejores modelos usando el Training Set y k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "l1_ratio=0.5\n",
    "normalize=False\n",
    "max_iter=100000\n",
    "warm_start=True\n",
    "modelos_a_probar = {\n",
    "    'Linear_Regression': {'modelo': make_pipeline(StandardScaler(), LinearRegression())},\n",
    "    'Linear_Regression degree 2': {'modelo': make_pipeline(\n",
    "        PolynomialFeatures(degree=2),\n",
    "        LinearRegression()\n",
    "    )},\n",
    "    'Linear_Regression degree 3': {'modelo': make_pipeline(\n",
    "        PolynomialFeatures(degree=3),\n",
    "        LinearRegression()\n",
    "    )},\n",
    "    'Linear_Regression degree 2 with normalization': {'modelo': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        PolynomialFeatures(degree=2),\n",
    "        LinearRegression()\n",
    "    )},\n",
    "    'Lasso': {'modelo': Lasso()},\n",
    "    'Ridge': {'modelo': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge()\n",
    "    )},\n",
    "    'Ridge degree 2': {'modelo': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        PolynomialFeatures(degree=2),\n",
    "        Ridge()\n",
    "    )},\n",
    "    'ElasticNet': {'modelo': make_pipeline(\n",
    "        StandardScaler(),\n",
    "        ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=max_iter, warm_start=warm_start)\n",
    "    )},\n",
    "    'SGD': {'modelo': make_pipeline(StandardScaler(), SGDRegressor())},\n",
    "    'SVR': {'modelo': make_pipeline(StandardScaler(), SVR())}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for nombre_modelo, dic_modelo in tqdm(modelos_a_probar.items(), desc='Realizando cross validation...'):\n",
    "    inicial = datetime.now()\n",
    "    modelo = dic_modelo['modelo']\n",
    "    dic_modelo['scores'] = cross_val_score(modelo, du.data[x_names], du.data[y_name], cv=5, scoring='r2')\n",
    "    tiempo_entrenamiento = (datetime.now() - inicial).total_seconds()\n",
    "    dic_modelo['tiempo_entrenamiento'] = tiempo_entrenamiento\n",
    "    dic_modelo['media'] = np.mean(dic_modelo['scores'])\n",
    "    dic_modelo['std'] = np.std(dic_modelo['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tabla_comparativa = pd.DataFrame(modelos_a_probar).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Comparativa R2')\n",
    "tabla_comparativa.drop(columns=['scores','modelo']).sort_values(by='media', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se escogerá el Ridge degree 2 pues aunque el regresor lineal de grado 2 con normalización obtuvo una media ligeramente mayor,\n",
    "el modelo Ridge de grado 2 lo superó bastante en tiempo de entrenamiento.\n",
    "Como segundo modelo para la optimización de hiper parámetros se utilizará Linear_Regression degree 2 with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mejor_modelo1 = modelos_a_probar['Ridge degree 2']['modelo']\n",
    "mejor_modelo2 = modelos_a_probar['Linear_Regression degree 2 with normalization']['modelo']\n",
    "mejor_modelo1.fit(du.data[x_names], du.data[y_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_predict = mejor_modelo1.predict(du.data[x_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_real = du.data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.linspace(y_real.min()*0.8, y_real.max()*1.2)\n",
    "fig1 = plot.scatter(pd.DataFrame({'y_real': y_real, 'y_pred': y_predict}), x='y_real', y='y_pred')\n",
    "fig2 = px.line(pd.DataFrame({'y_real': vector, 'y_pred': vector}), x='y_real', y='y_pred')\n",
    "plot.combine_plots(fig2, fig1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizacion de Hiper parametros (Hyper Parameter optimization)\n",
    "\n",
    "Se seleccionan solo los mejores modelos para realizar el ajuste de hiperparametros, ya que tiene una carga computacional alta.\n",
    "\n",
    "Al final se obtienen los parametros del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo el nombre de los parámetros del primer modelo\n",
    "mejor_modelo1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Obteniendo el nombre de los parámetros del segundo modelo\n",
    "mejor_modelo2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1_param_grid = [\n",
    "    {\n",
    "        'polynomialfeatures__interaction_only': [True, False],\n",
    "        'ridge__alpha': np.linspace(0.01, 6, 60)\n",
    "    }\n",
    "]\n",
    "\n",
    "model2_param_grid = [\n",
    "    {\n",
    "        'polynomialfeatures__interaction_only': [True, False],\n",
    "        'polynomialfeatures__include_bias': [True, False],\n",
    "        'linearregression__positive': [True, False]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs1 = GridSearchCV(mejor_modelo1, model1_param_grid, scoring='r2')\n",
    "gs2 = GridSearchCV(mejor_modelo2, model2_param_grid, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Realizando un grid search para el primer pipeline\n",
    "print('Grid search primer modelo')\n",
    "gs1.fit(du.data[x_names], du.data[y_name])\n",
    "\n",
    "print('Grid search segundo modelo')\n",
    "# Realizando un grid search para el segundo pipeline\n",
    "gs2.fit(du.data[x_names], du.data[y_name])\n",
    "print('Listo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Los mejores parámetros para el primer pipeline son:')\n",
    "print(gs1.best_params_)\n",
    "print()\n",
    "print('Los mejores parámetros para el segundo pipeline son:')\n",
    "print(gs2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "modelo_optimizado = make_pipeline(\n",
    "        make_column_transformer(\n",
    "            ('passthrough', [\n",
    "                'zipcode', 'grade', 'view', 'bathrooms', 'bedrooms', 'sqft_living15', 'waterfront', 'floors',\n",
    "                'sqft_lot', 'condition', 'sqft_lot15', 'sqft_living', 'fue_renovada', 'antiguedad_venta'\n",
    "            ])\n",
    "        ),\n",
    "        StandardScaler(),\n",
    "        PolynomialFeatures(degree=2, interaction_only=False),\n",
    "        Ridge(alpha=6.0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion final del modelo con el Test set\n",
    "\n",
    "Después de haber obtenido el flujo para la transformación de los datos, se empaquetó en un conjunto de pipelines de transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_validacion = du.load_data(du.raw_validation_path)\n",
    "set_entrenamiento = du.load_data(du.raw_train_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.procesamiento_datos import Preprocesamiento\n",
    "\n",
    "pval = Preprocesamiento(['price'], ['price'])\n",
    "set_validacion_transformado = set_validacion.pipe(preprocessing).pipe(pval.transform).pipe(build_features)[[variable_salida] + variables_entrada]\n",
    "set_entrenamiento_transformado = set_entrenamiento.pipe(preprocessing).pipe(pval.transform).pipe(build_features)[[variable_salida] + variables_entrada]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_optimizado.fit(set_entrenamiento_transformado[variables_entrada], set_entrenamiento_transformado[variable_salida])\n",
    "\n",
    "y_real_train, y_predict_train = set_entrenamiento_transformado[variable_salida], modelo_optimizado.predict(set_entrenamiento_transformado[variables_entrada])\n",
    "y_real_validation, y_predict_validation = set_validacion_transformado[variable_salida], modelo_optimizado.predict(set_validacion_transformado[variables_entrada])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_entrenamiento = r2_score(y_real_train, y_predict_train)\n",
    "r2_validacion = r2_score(y_real_validation, y_predict_validation)\n",
    "\n",
    "print(f'{r2_entrenamiento=:.2f}')\n",
    "print(f'{r2_validacion=:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una disminución en el puntaje R2 de 4 puntos porcentuales, esto muestra que el modelo no tiene overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion del Modelo (Deploying)\n",
    "Con el análisis básico y el ajuste hecho, comienza el trabajo real (ingeniería).\n",
    "\n",
    "El último paso para poner en produccion el modelo de prediccion sera:\n",
    "1. Entrenarlo en todo el conjunto de datos nuevamente, para hacer un uso completo de todos los datos disponibles.\n",
    "2. Usar los mejores parámetros encontrados mediante la validación cruzada, por supuesto. Esto es muy similar a lo que hicimos al principio, pero esta vez teniendo una idea de su comportamiento y estabilidad. La evaluación se realizó con honestidad, en divisiones distintas de entrenamiento / prueba.\n",
    "\n",
    "El predictor final se puede serializar y grabar en el disco, de modo que la próxima vez que lo usemos, podemos omitir todo el entrenamiento y usar el modelo capacitado directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import pickle # Esta es una libreria de serializacion nativa de python, puede tener problemas de seguridad\n",
    "\n",
    "df_completo = pd.read_csv(du.raw_path.joinpath('kc_house_dataDS.csv'), index_col=0, decimal='.')\n",
    "# df_transformado = df_completo.pipe(li.fit_transform).pipe(pre_pro.fit_transform).pipe(pval.fit_transform).pipe(build_features)\n",
    "# df_transformado = df_transformado[[variable_salida] + variables_entrada]\n",
    "_columnas_numericas = [columna for columna in df_completo.columns if columna != 'date']\n",
    "pre_pro = Preprocesamiento(['price', 'sqft_lot', 'sqft_lot15'], [])\n",
    "li = LimpiezaCalidad(_columnas_numericas)\n",
    "pda = ProcesamientoDatos()\n",
    "\n",
    "pipeline_tranformacion_prediccion = make_pipeline(\n",
    "    li, pre_pro, pda\n",
    ")\n",
    "\n",
    "pipeline_tranformacion_entrenamiento=make_pipeline(\n",
    "    li, pre_pro, pval, pda\n",
    ")\n",
    "\n",
    "df_transformado = pipeline_tranformacion_entrenamiento.fit_transform(df_completo)\n",
    "pipeline_tranformacion_prediccion.fit(df_completo)\n",
    "modelo_optimizado.fit(df_transformado[variables_entrada], df_transformado[variable_salida])\n",
    "\n",
    "# Guardando\n",
    "# garbar el modelo en un archivo\n",
    "joblib.dump(pda, du.model_path.with_stem('pda'))\n",
    "\n",
    "du.model = modelo_optimizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando la validación del modelo con los datos completos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "df = pd.read_csv(du.raw_path.joinpath('kc_house_dataDS.csv'), index_col=0, decimal='.')\n",
    "\n",
    "li = LimpiezaCalidad(_columnas_numericas)\n",
    "pre_pro = Preprocesamiento(['price', 'sqft_lot', 'sqft_lot15'], [])\n",
    "pda: ProcesamientoDatos = joblib.load(du.model_path.with_stem('pda'))\n",
    "\n",
    "pipeline_tranformacion_prediccion = make_pipeline(\n",
    "    li, pre_pro, pda\n",
    ")\n",
    "\n",
    "pipeline_tranformacion_validacion = make_pipeline(\n",
    "    li, pre_pro, pval, pda\n",
    ")\n",
    "model: Pipeline = joblib.load(du.model_path)\n",
    "\n",
    "df_transformado = pipeline_tranformacion_validacion.transform(df)\n",
    "y_predict = model.predict(df_transformado)\n",
    "y_real = df_transformado['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_outlier = y_real.isna()\n",
    "\n",
    "print(f'{r2_score(y_real[~is_outlier], y_predict[~is_outlier]):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [\n",
    "    plot.scatter(pd.DataFrame({'y_real_train': y_real_train, 'y_predict_train':y_predict_train}), x='y_real_train', y='y_predict_train'),\n",
    "    plot.scatter(pd.DataFrame({'y_real_train': y_real, 'y_predict_train':y_predict}), x='y_real_train', y='y_predict_train')\n",
    "]\n",
    "plots2 = [\n",
    "    plot.histogram(pd.DataFrame({'y_real_train': y_real_train, 'y_predict_train':y_predict_train}), x='y_real_train'),\n",
    "    plot.histogram(pd.DataFrame({'y_real': y_real, 'y_predict_train':y_predict}), x='y_real')\n",
    "]\n",
    "\n",
    "plot.grid_subplot(*plots, \n",
    "                  cols=2, \n",
    "                  titles=['División test-train', 'Dataset completo'],\n",
    "                  title='Scater plot entre datos de entrenamiento y dataset completo',\n",
    "                  height=500\n",
    "                 ).show()\n",
    "\n",
    "plot.grid_subplot(*plots2, \n",
    "                  cols=2, \n",
    "                  titles=['División test-train', 'Dataset completo'],\n",
    "                  title='Distribución entre datos de entrenamiento y dataset completo',\n",
    "                  height=500\n",
    "                 ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa una disminución en el R2 al entrenar con los datos completos, al revisar la distribución de los datos, se observa que en la eliminación de datos con el z-score, quedaron por fuera precios mayores de 2 millones, esto causa una disminución en el cálculo del R2. Esto será solucionado en la implementación del modelo en código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comunicacion de Resultados (Data Story Telling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico de dependencia parcial.\n",
    "Los gráficos de dependencia parcial son una forma de ver el impacto que tiene una variable en la respuesta. Se realizará el gráfico para las variables más importantes identificadas anteriormente: sqft_living, grade, sqft_above, sqft_living, bathrooms, view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import partial_dependence\n",
    "# from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from time import time\n",
    "\n",
    "features = ['sqft_living', 'sqft_lot', 'sqft_lot15', 'grade', 'view', 'bathrooms', 'bedrooms', 'sqft_living15', \n",
    "            'waterfront', 'floors', 'condition', 'fue_renovada', 'antiguedad_venta']\n",
    "\n",
    "x = PartialDependenceDisplay.from_estimator(model, df_transformado.drop(columns='price'),features= features)\n",
    "\n",
    "print('Computing partial dependence plots...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 15))\n",
    "x.plot(ax=ax)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que las siguientes variables no varían mucho el precio de venta de los hogares:\n",
    "- sqft_lot\n",
    "- floors\n",
    "- fue_renovada\n",
    "- bedrooms\n",
    "- bathrooms\n",
    "\n",
    "Las siguientes tienen un efecto moderado en la salida:\n",
    "- sqft_living\n",
    "- sqft_living15\n",
    "- condition\n",
    "- view\n",
    "\n",
    "Las siguientes variables tienen un fuerte efecto en el precio\n",
    "- waterfront \n",
    "- grade\n",
    "- antiguedad_venta\n",
    "\n",
    "La siguiente variable tiene un leve impacto negativo en el precio\n",
    "- sqft_lot15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las variables que mayor impacto tienen en el precio de los hogares son variables relacionadas con el área habitable y entorno, como por ejemplo, el diseño, el tamaño, la vista, utilizacion de los interiores, vista a fuentes hidricas.\n",
    "- Se encontró que el mejor modelo es un modelo de regresión lineal con regularización (Ridge) con características polinomiales de segundo orden obteniendo un R2 de 65%, esta disminución corresponde a una eliminación exceciva de outliers, lo cual será revisado posteriormente para la mejora del modelo.\n",
    "- Una correcta limpieza de outliers fue fundamental para mejorar los resultados del modelo.\n",
    "\n",
    "**Recomendaciones**\n",
    "Si actualmente se cuenta con una propiedad en Kansas y se tiene dinero disponible para invertir antes de realizar la venta, es recomendable antes de aumentar el tamaño del lote, invertir en la mejora del diseño de interior, que ayude a mejorar los espacios y aumentar el área habitable, pues se encontró que aunque la renovación aumenta el valor de las casas, si esta renovación no viene acompañada de un aumento en el valor estético de la misma, no se logran obtener los mejores beneficios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ayudas Y Referencias\n",
    "\n",
    "- https://medium.com/@joserzapata/paso-a-paso-en-un-proyecto-machine-learning-bcdd0939d387\n",
    "- [Proyecto de Principio a Final sobre readmision de pacientes con Diabetes](https://github.com/JoseRZapata/Readmission-ML-Project)\n",
    "\n",
    "- [a-complete-machine-learning-walk-through-in-python-part-one](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420)\n",
    "\n",
    "\n",
    "- [a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn](https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf#249d)\n",
    "\n",
    "- [a-data-science-for-good-machine-learning-project-walk-through-in-python-part-one](https://towardsdatascience.com/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc)\n",
    "\n",
    "- [Ejemplos de Kaggle](https://www.kaggle.com/kernels?sortBy=hotness&group=everyone&pageSize=20&language=Python&kernelType=Notebook)\n",
    "\n",
    "- [END to END ML from data colletion to deployment](https://medium.com/datadriveninvestor/end-to-end-machine-learning-from-data-collection-to-deployment-ce74f51ca203)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docente: [Jose R. Zapata](https://joserzapata.github.io)\n",
    "- https://joserzapata.github.io\n",
    "- https://twitter.com/joserzapata\n",
    "- https://www.linkedin.com/in/jose-ricardo-zapata-gonzalez/   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mXQHmtMbHJoh",
    "3aVUkwJVHJom",
    "9SfZHh6oHJor",
    "1S_iRK8rHJo0",
    "Ey5Lk8evHJo3",
    "0Y0UV-GVHJo6",
    "he4lxsEUHJpM",
    "h0FKIrJHHJpO",
    "jYPdefrhHJpS",
    "zUundEM6HJpT",
    "2GNrgJzHHJpW",
    "uRwep7LjHJpZ",
    "JCcM4XFbHJpd"
   ],
   "name": "Trabajo_Preparacion_Datos_JoseR_Zapata.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Contenido",
   "title_sidebar": "Contenido",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "447c0f0993c7e50bc10ddc9bd7e362220c6ef046a7e5a6eb2fbe70cad79928d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
